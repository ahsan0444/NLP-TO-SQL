{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsan0444/NLP-TO-SQL/blob/main/text_to_sql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYNyOjhw-6Iw"
      },
      "source": [
        "# **Installation and Setup**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zgBqwRTGq21c"
      },
      "outputs": [],
      "source": [
        "!pip install openai transformers datasets peft trl huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGd6w15AswZk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['HF_TOKEN'] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "wXK2-4n9sq0x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login, logout\n",
        "\n",
        "login(token=os.environ['HF_TOKEN'], add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyGqCEr3_IHm"
      },
      "source": [
        "# **Load the Base Model and Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXBGb3Hnqn7S"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "model_name = 't5-small'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "original_model = original_model.to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gc3dgxAGty_u"
      },
      "outputs": [],
      "source": [
        "original_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CI7YBso_MH3"
      },
      "source": [
        "# **Prepare Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8G59PmWuX-F"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, interleave_datasets\n",
        "\n",
        "# Load and split the first dataset\n",
        "dataset_scc_train = load_dataset(\"b-mc2/sql-create-context\", split='train[:80%]')\n",
        "dataset_scc_test  = load_dataset(\"b-mc2/sql-create-context\", split='train[-20%:-10%]')\n",
        "dataset_scc_val   = load_dataset(\"b-mc2/sql-create-context\", split='train[-10%:]')\n",
        "\n",
        "# Load, preprocess, and split the second dataset\n",
        "dataset_tts_train = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[:80%]')\n",
        "dataset_tts_train = (dataset_tts_train\n",
        "                     .remove_columns(['source', 'text'])\n",
        "                     .rename_columns({'instruction': 'question', 'input': 'context', 'response': 'answer'}))\n",
        "\n",
        "dataset_tts_test = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[-20%:-10%]')\n",
        "dataset_tts_test = (dataset_tts_test\n",
        "                    .remove_columns(['source', 'text'])\n",
        "                    .rename_columns({'instruction': 'question', 'input': 'context', 'response': 'answer'}))\n",
        "\n",
        "dataset_tts_val = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[-10%:]')\n",
        "dataset_tts_val = (dataset_tts_val\n",
        "                   .remove_columns(['source', 'text'])\n",
        "                   .rename_columns({'instruction': 'question', 'input': 'context', 'response': 'answer'}))\n",
        "\n",
        "# Load and split the third dataset\n",
        "dataset_ks_train = load_dataset(\"knowrohit07/know_sql\", split='validation[:80%]')\n",
        "dataset_ks_test  = load_dataset(\"knowrohit07/know_sql\", split='validation[-20%:-10%]')\n",
        "dataset_ks_val   = load_dataset(\"knowrohit07/know_sql\", split='validation[-10%:]')\n",
        "\n",
        "# Interleave the datasets to create a combined DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': interleave_datasets([dataset_scc_train, dataset_tts_train, dataset_ks_train]),\n",
        "    'test': interleave_datasets([dataset_scc_test, dataset_tts_test, dataset_ks_test]),\n",
        "    'validation': interleave_datasets([dataset_scc_val, dataset_tts_val, dataset_ks_val])\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSqPulmD_TNI"
      },
      "source": [
        "# **Tokenization Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwbnjQ0mukWy"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    # Define the prompt structure\n",
        "    start_prompt = \"Tables:\\n\"\n",
        "    middle_prompt = \"\\n\\nQuestion:\\n\"\n",
        "    end_prompt = \"\\n\\nAnswer:\\n\"\n",
        "\n",
        "    # Create prompts by combining context and question\n",
        "    data_zip = zip(example['context'], example['question'])\n",
        "    prompt = [\n",
        "        start_prompt + context + middle_prompt + question + end_prompt\n",
        "        for context, question in data_zip\n",
        "    ]\n",
        "\n",
        "    # Tokenize the prompts and answers\n",
        "    example['input_ids'] = tokenizer(\n",
        "        prompt,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    example['labels'] = tokenizer(\n",
        "        example['answer'],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# Apply the tokenize function across all dataset splits\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "# Remove the original columns\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['question', 'context', 'answer'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJintd3c_gtO"
      },
      "source": [
        "# **Zero-Shot Model Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U__40TpavAvZ"
      },
      "outputs": [],
      "source": [
        "# Initialize the index for selecting an example\n",
        "index = 0\n",
        "\n",
        "# Extract the question, context, and answer from the test split of the dataset\n",
        "question = dataset['test'][index]['question']\n",
        "context = dataset['test'][index]['context']\n",
        "answer = dataset['test'][index]['answer']\n",
        "\n",
        "# Construct the input prompt\n",
        "prompt = f\"\"\"Tables:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the prompt and move the tensors to GPU (if available)\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "# inputs = inputs.to('cuda')\n",
        "\n",
        "# Generate the model's output\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Print the results\n",
        "dash_line = '-' * 100\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN ANSWER:\\n{answer}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAdg_WYQ_nck"
      },
      "source": [
        "# **Fine-Tuning Setup with PEFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RmoQl03wL3Y"
      },
      "outputs": [],
      "source": [
        "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "# finetuned_model = finetuned_model.to('cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxi7zdyMyzsL"
      },
      "outputs": [],
      "source": [
        "# Disable cache to improve training speed.\n",
        "finetuned_model.config.use_cache = False\n",
        "\n",
        "# Set the temperature for pretraining to 1.\n",
        "finetuned_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p3kwtwiuw7v6"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# Define the PEFT configuration.\n",
        "peft_config = LoraConfig(\n",
        "    # Set the rank of the LoRA projection matrix.\n",
        "    r=8,\n",
        "\n",
        "    # Set the alpha parameter for the LoRA projection matrix.\n",
        "    lora_alpha=16,\n",
        "\n",
        "    # Set the dropout rate for the LoRA projection matrix.\n",
        "    lora_dropout=0.05,\n",
        "\n",
        "    # Set the bias term to \"none\".\n",
        "    bias=\"none\",\n",
        "\n",
        "    # Set the task type to \"CAUSAL_LM\".\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgvdwlUf_qkk"
      },
      "source": [
        "# **Training Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3BS7YE8apUn"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnIRcew6vZxs"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from trl import SFTTrainer\n",
        "import time\n",
        "\n",
        "\n",
        "# Define the output directory with a timestamp for uniqueness\n",
        "output_dir = f'./sql-training-{int(time.time())}'\n",
        "\n",
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=5e-3,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,  # Batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # Batch size per device during evaluation\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    eval_strategy='steps',           # Updated argument name for evaluation strategy\n",
        "    eval_steps=500,                  # Number of steps between evaluations\n",
        "    fp16=True,                       # Enable fp16 training\n",
        "    optim=\"paged_adamw_32bit\",       # Set the optimizer to use\n",
        "    gradient_accumulation_steps=2,   # Set the number of gradient accumulation steps\n",
        "    lr_scheduler_type=\"cosine\",      # Set the learning rate scheduler type\n",
        "    save_strategy=\"epoch\"            # Set the save strategy\n",
        ")\n",
        "\n",
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=finetuned_model,          # Set the model to be trained\n",
        "    train_dataset=tokenized_datasets['train'],  # Set the training dataset\n",
        "    eval_dataset=tokenized_datasets['validation'],  # Set evaluation dataset\n",
        "    peft_config=peft_config,        # Set the PEFT configuration\n",
        "    args=training_args,             # Set the training arguments\n",
        "    tokenizer=tokenizer,            # Set the tokenizer\n",
        "    packing=False,                  # Disable packing\n",
        "    max_seq_length=1024             # Set the maximum sequence length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UGzCBHWxzfSz"
      },
      "outputs": [],
      "source": [
        "trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3E2zRNKySBY"
      },
      "outputs": [],
      "source": [
        "# Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDsO9M2E_umr"
      },
      "source": [
        "# ** Evaluate the Fine-Tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQveUU_-7KRx"
      },
      "outputs": [],
      "source": [
        "# Select the index for the example (uncomment the line below to use the last 200 examples)\n",
        "index = 0\n",
        "# index = len(dataset['test']) - 200\n",
        "\n",
        "# Extract the question, context, and answer for the selected example\n",
        "question = dataset['test'][index]['question']\n",
        "context = dataset['test'][index]['context']\n",
        "answer = dataset['test'][index]['answer']\n",
        "\n",
        "# Construct the input prompt\n",
        "prompt = f\"\"\"Tables:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the prompt and move the tensors to GPU\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "inputs = inputs.to('cuda')\n",
        "\n",
        "# Generate the model's output\n",
        "output = tokenizer.decode(\n",
        "    finetuned_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Print the results with visual separators\n",
        "dash_line = '-' * 100\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN ANSWER:\\n{answer}\\n')\n",
        "print(dash_line)\n",
        "print(f'FINE-TUNED MODEL - ZERO SHOT:\\n{output}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPV00UwYE/WCS34R114Kbix",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}